This lesson provides 15 hands-on exercises across 7 modules, bridging the gap between understanding database concepts and building database applications from requirements. Three core skills are practiced throughout: **data modeling** (translating real-world entities into SQLAlchemy models with correct types, constraints, and relationships), **database debugging** (finding bugs that don't crash â€” wrong column types, missing constraints, broken relationships, and transaction safety holes), and **production deployment** (configuring Neon PostgreSQL with proper security, connection pooling, and hybrid verification). Every exercise uses the six-step Database Development Framework: Model, Connect, Operate, Protect, Verify, Deploy.

Each module now defines **minimum verification outputs** and expected evidence artifacts (for example: `crud-test-output.txt`, `transaction-drill.txt`, `security-proof.txt`, `mismatch-policy-result.json`, `CAPSTONE-EVIDENCE.md`). This ensures exercise completion predicts chapter mastery instead of relying on subjective confidence.

The modules progress from data modeling fundamentals (Module 1: Data Modeling) through CRUD operations with session management (Module 2: CRUD Operations), relationship configuration and navigation (Module 3: Relationships & Navigation), transaction safety with commit/rollback boundaries (Module 4: Transaction Safety), cloud deployment and connection troubleshooting (Module 5: Cloud Deployment & Security), and hybrid SQL + independent verification with tool selection (Module 6: Hybrid Verification & Tool Selection), culminating in three capstone projects that build genuinely useful database applications.

Students assess their work using a five-criteria rubric covering data modeling, session management, relationship design, transaction safety, and debugging accuracy. These exercises prepare students for the Chapter Quiz by applying every pattern from Lessons 0-8, and connect to the broader Part 2 tool choice story where bash, Python, SQL, and hybrid verification form a complete foundation for autonomous agent workflows.
